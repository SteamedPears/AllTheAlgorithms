\chapter{Dynamic Programming}

So far we have considered two major strategies in algorithms design:
greedy, in which we repeatedly take the local optimum choice; and
divide and conquer, in which we divide the greater problem into
similar subproblems and recurse.

There may be problems for which these strategies are suboptimal.  In
which case, we have a third strategy which may be of use: dynamic
programming.  This strategy divides the problem into sub problems, but
rather than recursing on each sub problem individually, we identify
easy to compute base cases from which we can build towards the
solution to the larger problem, storing the results of previous
computations to use in later computations.  This technique differs
from a similar technique called ``memoization'' which computes
recursively top-down, whereas dynamic programming begins at the base
case(s) and works up.

Dynamic programming has these important steps:

\begin{enumerate}
\item Determine structure of optimal solution
\item Set up recurrences for optimal solution
\item Solve recurrences bottom-up
\item Construct the optimal solution
\end{enumerate}

And the final step is most often neglected since we can typically add
some small amount of information to the process so we can trivially
reconstruct the optimal solution.

\section{Matrix Chain Multiplication}

Assuming knowledge of matrices and how they are multiplied.

The problem is to find the way to multiply $n$ matrices $A_1,...,A_n$
with dimensions $p_0,...,p_n$ (e.g. $A_i$ has dimensions $p_{i-1}
\cross p_i$) using the least number of calculations.  Notice that
multiplying $A_iA_{i+1}$ takes $p_{i-1}p_ip_{i+1}$ calculations.

We start by determining the structure of the optimal solution.
Observe that the optimal solution will necessarily involve splitting
$A_1A_2...A_n$ into two subproblems at some optimally chosen
$A_kA_{k+1}$ so we multiply $A_1...A_k$ then $A_k...A_n$ and then
multiply the results together.  If we define the function $m(i,j)$ to
be the minimum cost of multiplying $A_i...A_j$, then the cost of the
optimal solution is $m(1,n) = m(1,k) + m(k+1,n) + p_0p_kp_n$.

We then define the recurrence $m(i,i) = 0$ and $m(i,j) = min \{ m(i,k) +
m(k+1,j) + p_{i-1}p_kp_j \}$ for all $k$ such that $i < k \leq j$.

We then compute all $m(i,i)$ for $1 \leq i \leq n$, then all
$m(i,i+1)$, $m(i,i+2)$, ... until we have calculated $m(1,n)$.

\section{Longest Common Subsequence}

\section{Optimal Triangulation of a Convex Polygon}

% page 91 in John's notes

First some definitions.  A \emph{polygon} is a list of vertices
$(v_1,...,v_n)$ such that for any $v_i$, there exists an edge
$(v_i,v_{i+1})$ and also there exists an edge $(v_1,v_n)$.  A polygon
is said to be \emph{convex} if any line passing through the polygon
crosses the edges of the polygon at most twice.  A \emph{chord} is an
edge between two non-adjacent vertices in a polygon.  A
\emph{triangulation} is a set of chords which divide a polygon into
triangles.

The problem is to build a triangulation of a given convex polygon
which minimizes total edge length.  We define the function $w(a,b,c)$
to be the weight of the triangle $(v_a,v_b,v_c)$, which in this case
will be the length of the edges $(v_a,v_b)$, $(v_b,v_c)$, and
$(v_c,v_a)$.  We also define the function $t(a,b)$ to be the optimal
triangulation of points $(v_a,...,v_b)$.  We would like to solve
$t(1,n)$.

We start by defining the structure of an optimal solution.  Notice
that the optimal triangulation contains the triangle $(v_1,v_k,v_n)$
for some $k$.  The cost of this triangulation is $t(1,k) + t(k,n) +
w(1,k,n)$.

We then define the recurrence $t(i,i+1) = 0$ for all $i$, and $t(i,j)
= min \{ t(i,k) + t(k,j) + w(i,k,j) \}$ for all $k$ such that $i < k < j$.

We then compute all $t(i,i+1)$, then all $t(i,i+2)$, $t(i,i+3)$,
... until we have calculated $t(1,n)$.

\hypertarget{sec:floyd_warshall}{\section{All-Pairs Shortest Path (Floyd-Warshall)}}

\section{String Edit Distance}

